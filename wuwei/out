# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[(1024, 1024), "float16"], B: T.Buffer[(1024, 1024), "float16"], C: T.Buffer[(1024, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.tensor_core_enabled":"1", "warp_execution":1})
            C_reindex_wmma_accumulator = T.alloc_buffer([1024, 1024], dtype="float32", scope="wmma.accumulator")
            A_reindex_shared = T.alloc_buffer([1024, 1024], dtype="float16", scope="shared")
            B_reindex_shared = T.alloc_buffer([1024, 1024], dtype="float16", scope="shared")
            A_reindex_shared_wmma_matrix_a = T.alloc_buffer([1024, 1024], dtype="float16", scope="wmma.matrix_a")
            B_reindex_shared_wmma_matrix_b = T.alloc_buffer([1024, 1024], dtype="float16", scope="wmma.matrix_b")
            for ax_0_0_ax_0_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
                for ax_0_1_ax_0_1_fused in T.thread_binding(8, thread="blockIdx.y"):
                    for ax_0_2_ax_0_2_fused in T.thread_binding(4, thread="threadIdx.y"):
                        for ax_0_3_init in T.serial(4):
                            with T.block("update_o_init"):
                                bv_o = T.axis.spatial(64, ax_0_0_ax_0_0_fused // 8 * 16 + ax_0_1_ax_0_1_fused // 2 * 4 + ax_0_2_ax_0_2_fused)
                                bv_o_1 = T.axis.spatial(64, ax_0_0_ax_0_0_fused % 8 * 8 + ax_0_1_ax_0_1_fused % 2 * 4 + ax_0_3_init)
                                T.reads()
                                T.writes(C_reindex_wmma_accumulator[bv_o * 16 : bv_o * 16 + 16, bv_o_1 * 16 : bv_o_1 * 16 + 16])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32})
                                for ax_1_0, ax_1_0_1 in T.grid(1, 1):
                                    with T.block("update_init_o"):
                                        bv_init_o = T.axis.spatial(1, 0)
                                        bv_init_o_1 = T.axis.spatial(1, 0)
                                        T.reads()
                                        T.writes(C_reindex_wmma_accumulator[bv_o * 16 : bv_o * 16 + 16, bv_o_1 * 16 : bv_o_1 * 16 + 16])
                                        C_1 = T.match_buffer(C_reindex_wmma_accumulator[bv_o * 16 : bv_o * 16 + 16, bv_o_1 * 16 : bv_o_1 * 16 + 16], [16, 16], dtype="float32", scope="wmma.accumulator", offset_factor=16)
                                        T.evaluate(T.tvm_fill_fragment(C_1.data, 16, 16, 16, C_1.elem_offset // 256 + C_1.elem_offset % 256 // 16, T.float32(0), dtype="handle"))
                        for ax_0_0 in T.serial(16, annotations={"software_pipeline_order":[0, 1, 3, 4, 5, 2, 6], "software_pipeline_stage":[0, 0, 0, 0, 0, 1, 1]}):
                            with T.block("A_reindex_shared"):
                                v0, v1, v2 = T.axis.remap("SSS", [ax_0_0_ax_0_0_fused, ax_0_1_ax_0_1_fused, ax_0_0])
                                T.reads(A[v0 // 8 * 256 + v1 // 2 * 64 : v0 // 8 * 256 + v1 // 2 * 64 + 64, v2 * 64 : v2 * 64 + 64])
                                T.writes(A_reindex_shared[v0 // 8 * 256 + v1 // 2 * 64 : v0 // 8 * 256 + v1 // 2 * 64 + 64, v2 * 64 : v2 * 64 + 64])
                                T.block_attr({"auto_copy":1, "double_buffer_scope":0, "local_stage":1, "vector_bytes":8})
                                for ax0, ax1 in T.grid(64, 64):
                                    A_reindex_shared[v0 // 8 * 256 + v1 // 2 * 64 + ax0, v2 * 64 + ax1] = A[v0 // 8 * 256 + v1 // 2 * 64 + ax0, v2 * 64 + ax1]
                            with T.block("B_reindex_shared"):
                                v0, v1, v2 = T.axis.remap("SSS", [ax_0_0_ax_0_0_fused, ax_0_1_ax_0_1_fused, ax_0_0])
                                T.reads(B[v0 % 8 * 128 + v1 % 2 * 64 : v0 % 8 * 128 + v1 % 2 * 64 + 64, v2 * 64 : v2 * 64 + 64])
                                T.writes(B_reindex_shared[v0 % 8 * 128 + v1 % 2 * 64 : v0 % 8 * 128 + v1 % 2 * 64 + 64, v2 * 64 : v2 * 64 + 64])
                                T.block_attr({"auto_copy":1, "double_buffer_scope":0, "local_stage":1, "vector_bytes":16})
                                for ax0, ax1 in T.grid(64, 64):
                                    B_reindex_shared[v0 % 8 * 128 + v1 % 2 * 64 + ax0, v2 * 64 + ax1] = B[v0 % 8 * 128 + v1 % 2 * 64 + ax0, v2 * 64 + ax1]
                            for ax_0_1 in T.serial(2, annotations={"software_pipeline_order":[0, 1, 2], "software_pipeline_stage":[0, 0, 1]}):
                                with T.block("A_reindex_shared_wmma.matrix_a"):
                                    v0, v1, v2, v3, v4 = T.axis.remap("SSSSS", [ax_0_0_ax_0_0_fused, ax_0_1_ax_0_1_fused, ax_0_2_ax_0_2_fused, ax_0_0, ax_0_1])
                                    T.reads(A_reindex_shared[v0 // 8 * 256 + v1 // 2 * 64 + v2 * 16 : v0 // 8 * 256 + v1 // 2 * 64 + v2 * 16 + 16, v3 * 64 + v4 * 32 : v3 * 64 + v4 * 32 + 32])
                                    T.writes(A_reindex_shared_wmma_matrix_a[v0 // 8 * 256 + v1 // 2 * 64 + v2 * 16 : v0 // 8 * 256 + v1 // 2 * 64 + v2 * 16 + 16, v3 * 64 + v4 * 32 : v3 * 64 + v4 * 32 + 32])
                                    T.block_attr({"auto_copy":1})
                                    for ax0, ax1 in T.grid(16, 32):
                                        A_reindex_shared_wmma_matrix_a[v0 // 8 * 256 + v1 // 2 * 64 + v2 * 16 + ax0, v3 * 64 + v4 * 32 + ax1] = A_reindex_shared[v0 // 8 * 256 + v1 // 2 * 64 + v2 * 16 + ax0, v3 * 64 + v4 * 32 + ax1]
                                with T.block("B_reindex_shared_wmma.matrix_b"):
                                    v0, v1, v2, v3 = T.axis.remap("SSSS", [ax_0_0_ax_0_0_fused, ax_0_1_ax_0_1_fused, ax_0_0, ax_0_1])
                                    T.reads(B_reindex_shared[v0 % 8 * 128 + v1 % 2 * 64 : v0 % 8 * 128 + v1 % 2 * 64 + 64, v2 * 64 + v3 * 32 : v2 * 64 + v3 * 32 + 32])
                                    T.writes(B_reindex_shared_wmma_matrix_b[v0 % 8 * 128 + v1 % 2 * 64 : v0 % 8 * 128 + v1 % 2 * 64 + 64, v2 * 64 + v3 * 32 : v2 * 64 + v3 * 32 + 32])
                                    T.block_attr({"auto_copy":1})
                                    for ax0, ax1 in T.grid(64, 32):
                                        B_reindex_shared_wmma_matrix_b[v0 % 8 * 128 + v1 % 2 * 64 + ax0, v2 * 64 + v3 * 32 + ax1] = B_reindex_shared[v0 % 8 * 128 + v1 % 2 * 64 + ax0, v2 * 64 + v3 * 32 + ax1]
                                for ax_0_3, ax_0_3_1, ax_0_2, ax_0_4, ax_0_4_1 in T.grid(1, 4, 2, 1, 1):
                                    with T.block("update_o_update"):
                                        bv_o_2 = T.axis.spatial(64, ax_0_0_ax_0_0_fused // 8 * 16 + ax_0_1_ax_0_1_fused // 2 * 4 + ax_0_2_ax_0_2_fused)
                                        bv_o_3 = T.axis.spatial(64, ax_0_0_ax_0_0_fused % 8 * 8 + ax_0_1_ax_0_1_fused % 2 * 4 + ax_0_3_1)
                                        bv_o_4 = T.axis.reduce(64, ax_0_0 * 4 + ax_0_1 * 2 + ax_0_2)
                                        T.reads(C_reindex_wmma_accumulator[bv_o_2 * 16 : bv_o_2 * 16 + 16, bv_o_3 * 16 : bv_o_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[bv_o_2 * 16 : bv_o_2 * 16 + 16, bv_o_4 * 16 : bv_o_4 * 16 + 16], B_reindex_shared_wmma_matrix_b[bv_o_3 * 16 : bv_o_3 * 16 + 16, bv_o_4 * 16 : bv_o_4 * 16 + 16])
                                        T.writes(C_reindex_wmma_accumulator[bv_o_2 * 16 : bv_o_2 * 16 + 16, bv_o_3 * 16 : bv_o_3 * 16 + 16])
                                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32})
                                        for ax_1_0, ax_1_0_2, ax_1_0_3 in T.grid(1, 1, 1):
                                            with T.block("update_o"):
                                                bv_o_5 = T.axis.spatial(1, 0)
                                                bv_o_6 = T.axis.spatial(1, 0)
                                                bv_o_7 = T.axis.reduce(1, 0)
                                                T.reads(C_reindex_wmma_accumulator[bv_o_2 * 16 : bv_o_2 * 16 + 16, bv_o_3 * 16 : bv_o_3 * 16 + 16], A_reindex_shared_wmma_matrix_a[bv_o_2 * 16 : bv_o_2 * 16 + 16, bv_o_4 * 16 : bv_o_4 * 16 + 16], B_reindex_shared_wmma_matrix_b[bv_o_3 * 16 : bv_o_3 * 16 + 16, bv_o_4 * 16 : bv_o_4 * 16 + 16])
                                                T.writes(C_reindex_wmma_accumulator[bv_o_2 * 16 : bv_o_2 * 16 + 16, bv_o_3 * 16 : bv_o_3 * 16 + 16])
                                                A_1 = T.match_buffer(A_reindex_shared_wmma_matrix_a[bv_o_2 * 16 : bv_o_2 * 16 + 16, bv_o_4 * 16 : bv_o_4 * 16 + 16], [16, 16], dtype="float16", scope="wmma.matrix_a", offset_factor=16)
                                                B_1 = T.match_buffer(B_reindex_shared_wmma_matrix_b[bv_o_3 * 16 : bv_o_3 * 16 + 16, bv_o_4 * 16 : bv_o_4 * 16 + 16], [16, 16], dtype="float16", scope="wmma.matrix_b", offset_factor=16)
                                                C_2 = T.match_buffer(C_reindex_wmma_accumulator[bv_o_2 * 16 : bv_o_2 * 16 + 16, bv_o_3 * 16 : bv_o_3 * 16 + 16], [16, 16], dtype="float32", scope="wmma.accumulator", offset_factor=16)
                                                T.evaluate(T.tvm_mma_sync(C_2.data, C_2.elem_offset // 256 + C_2.elem_offset % 256 // 16, A_1.data, A_1.elem_offset // 256, B_1.data, B_1.elem_offset // 256, C_2.data, C_2.elem_offset // 256 + C_2.elem_offset % 256 // 16, dtype="handle"))
                        with T.block("C_reindex_wmma.accumulator"):
                            v0, v1, v2 = T.axis.remap("SSS", [ax_0_0_ax_0_0_fused, ax_0_1_ax_0_1_fused, ax_0_2_ax_0_2_fused])
                            T.reads(C_reindex_wmma_accumulator[v0 // 8 * 256 + v1 // 2 * 64 + v2 * 16 : v0 // 8 * 256 + v1 // 2 * 64 + v2 * 16 + 16, v0 % 8 * 128 + v1 % 2 * 64 : v0 % 8 * 128 + v1 % 2 * 64 + 64])
                            T.writes(C[v0 // 8 * 256 + v1 // 2 * 64 + v2 * 16 : v0 // 8 * 256 + v1 // 2 * 64 + v2 * 16 + 16, v0 % 8 * 128 + v1 % 2 * 64 : v0 % 8 * 128 + v1 % 2 * 64 + 64])
                            T.block_attr({"auto_copy":1, "vector_bytes":16})
                            for ax0, ax1 in T.grid(16, 64):
                                C[v0 // 8 * 256 + v1 // 2 * 64 + v2 * 16 + ax0, v0 % 8 * 128 + v1 % 2 * 64 + ax1] = C_reindex_wmma_accumulator[v0 // 8 * 256 + v1 // 2 * 64 + v2 * 16 + ax0, v0 % 8 * 128 + v1 % 2 * 64 + ax1]
    

b0 = sch.get_block(name="update", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
b2 = sch.reindex(block=b0, buffer_index=0, is_write_index=True)
b3 = sch.reindex(block=b0, buffer_index=0, is_write_index=False)
b4 = sch.reindex(block=b0, buffer_index=1, is_write_index=False)
sch.transform_block_layout(block=b2, index_map=lambda vi_l, vj_l, vk_l: (vi_l, vj_l, vk_l, ))
sch.transform_block_layout(block=b3, index_map=lambda vi_l, vj_l, vk_l: (vi_l, vj_l, vk_l, ))
sch.transform_block_layout(block=b4, index_map=lambda vi_l, vj_l, vk_l: (vi_l, vj_l, vk_l, ))
sch.transform_block_layout(block=b0, index_map=lambda vi_l, vj_l, vk_l: (vi_l, vj_l, vk_l, ))
sch.transform_layout(block=b0, buffer_index=0, buffer_index_type=write, index_map=lambda vi_l, vj_l: (vi_l, vj_l, ))
sch.transform_layout(block=b0, buffer_index=1, buffer_index_type=read, index_map=lambda vj_l, vk_l: (vj_l, vk_l, ))
sch.transform_layout(block=b0, buffer_index=0, buffer_index_type=read, index_map=lambda vi_l, vk_l: (vi_l, vk_l, ))
l5, l6, l7 = sch.get_loops(block=b0)
l8, l9 = sch.split(loop=l7, factors=[64, 16])
l10, l11 = sch.split(loop=l6, factors=[64, 16])
l12, l13 = sch.split(loop=l5, factors=[64, 16])
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b0)
sch.reorder(l16, l18, l13, l11, l9)
b20 = sch.blockize(loop=l13)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_sync")
sch.annotate(block_or_loop=b20, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_fill")
b21 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b21, ann_key="meta_schedule.tensor_core_enabled", ann_val="1")
b22 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b22, ann_key="warp_execution", ann_val=1)
l23, l24, l25 = sch.get_loops(block=b20)
v26, v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l23, n=5, max_innermost_factor=4, decision=[4, 4, 4, 1, 1])
l31, l32, l33, l34, l35 = sch.split(loop=l23, factors=[v26, v27, v28, v29, v30])
v36, v37, v38, v39, v40 = sch.sample_perfect_tile(loop=l24, n=5, max_innermost_factor=4, decision=[8, 2, 1, 4, 1])
l41, l42, l43, l44, l45 = sch.split(loop=l24, factors=[v36, v37, v38, v39, v40])
v46, v47, v48 = sch.sample_perfect_tile(loop=l25, n=3, max_innermost_factor=4, decision=[16, 2, 2])
l49, l50, l51 = sch.split(loop=l25, factors=[v46, v47, v48])
sch.reorder(l31, l41, l32, l42, l33, l43, l49, l50, l34, l44, l51, l35, l45)
l52 = sch.fuse(l31, l41)
sch.bind(loop=l52, thread_axis="blockIdx.x")
l53 = sch.fuse(l32, l42)
sch.bind(loop=l53, thread_axis="blockIdx.y")
l54 = sch.fuse(l33, l43)
sch.bind(loop=l54, thread_axis="threadIdx.y")
sch.annotate(block_or_loop=b20, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b20, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b55 = sch.write_at(loop=l54, block=b20, write_buffer_index=0, storage_scope="wmma.accumulator")
sch.reverse_compute_inline(block=b2)
v56 = sch.sample_categorical(candidates=[4, 8, 16], probs=[0.33333333333333331, 0.33333333333333331, 0.33333333333333331], decision=2)
sch.annotate(block_or_loop=b55, ann_key="vector_bytes", ann_val=v56)
b57 = sch.read_at(loop=l49, block=b20, read_buffer_index=0, storage_scope="shared")
v58 = sch.sample_categorical(candidates=[4, 8, 16], probs=[0.33333333333333331, 0.33333333333333331, 0.33333333333333331], decision=1)
sch.annotate(block_or_loop=b57, ann_key="vector_bytes", ann_val=v58)
sch.annotate(block_or_loop=b57, ann_key="local_stage", ann_val=1)
sch.annotate(block_or_loop=b57, ann_key="double_buffer_scope", ann_val=0)
b59 = sch.read_at(loop=l49, block=b20, read_buffer_index=1, storage_scope="shared")
v60 = sch.sample_categorical(candidates=[4, 8, 16], probs=[0.33333333333333331, 0.33333333333333331, 0.33333333333333331], decision=2)
sch.annotate(block_or_loop=b59, ann_key="vector_bytes", ann_val=v60)
sch.annotate(block_or_loop=b59, ann_key="local_stage", ann_val=1)
sch.annotate(block_or_loop=b59, ann_key="double_buffer_scope", ann_val=0)
b61 = sch.read_at(loop=l50, block=b20, read_buffer_index=0, storage_scope="wmma.matrix_a")
b62 = sch.read_at(loop=l50, block=b20, read_buffer_index=1, storage_scope="wmma.matrix_b")
sch.compute_inline(block=b3)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=l50, ann_key="software_pipeline_stage", ann_val=[0, 0, 1])
sch.annotate(block_or_loop=l50, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l49, ann_key="software_pipeline_stage", ann_val=[0, 0, 0, 0, 0, 1, 1])
sch.annotate(block_or_loop=l49, ann_key="software_pipeline_order", ann_val=[0, 1, 3, 4, 5, 2, 6])
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66, b67, b68, b69, b70 = sch.get_child_blocks(b64)
l71, l72, l73, l74 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l71, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l71, ann_key="pragma_unroll_explicit", ann_val=1)
l75, l76, l77, l78 = sch.get_loops(block=b66)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l79, l80, l81, l82, l83 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l84, l85, l86, l87, l88 = sch.get_loops(block=b68)
sch.annotate(block_or_loop=l84, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l84, ann_key="pragma_unroll_explicit", ann_val=1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="update_o", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
sch.unannotate(block_or_loop=b113, ann_key="meta_schedule.auto_tensorize")
sch.unannotate(block_or_loop=b102, ann_key="meta_schedule.auto_tensorize")
b114, = sch.get_child_blocks(b113)
sch.annotate(block_or_loop=b114, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_fill")
b115 = sch.get_block(name="update_init", func_name="main")
sch.unannotate(block_or_loop=b115, ann_key="meta_schedule.auto_tensorize")
l116, l117 = sch.get_loops(block=b115)
l118, l119 = sch.split(loop=l117, factors=[1, 16])
l120, l121 = sch.split(loop=l116, factors=[1, 16])
l122, l123, l124, l125 = sch.get_loops(block=b115)
sch.reorder(l124, l121, l119)
sch.tensorize(block_or_loop=l121, tensor_intrin="wmma_fill")
b126 = sch.get_block(name="update", func_name="main")
sch.unannotate(block_or_loop=b126, ann_key="meta_schedule.auto_tensorize")
l127, l128, l129 = sch.get_loops(block=b126)
l130, l131 = sch.split(loop=l129, factors=[1, 16])
l132, l133 = sch.split(loop=l128, factors=[1, 16])
l134, l135 = sch.split(loop=l127, factors=[1, 16])
l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b126)
sch.reorder(l138, l140, l135, l133, l131)
sch.tensorize(block_or_loop=l135, tensor_intrin="wmma_sync")
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
#include <cuda_fp16.h>
__device__ half max(half a, half b)
{
  return __hgt(__half(a), __half(b)) ? a : b;
}
__device__ half min(half a, half b)
{
  return __hlt(__half(a), __half(b)) ? a : b;
}
#else

typedef unsigned short uint16_t;
typedef unsigned char uint8_t;
typedef signed char int8_t;
typedef int int32_t;
typedef unsigned long long uint64_t;
typedef unsigned int uint32_t;

#define TVM_FORCE_INLINE inline __attribute__((always_inline))
#define TVM_XINLINE TVM_FORCE_INLINE __device__ __host__
#define TVM_ALIGNED(x) __attribute__ ((aligned(x)))
#define TVM_HALF_OPERATOR(RTYPE, OP)                              \
  TVM_XINLINE RTYPE operator OP (half a, half b) {                \
    return RTYPE(float(a) OP float(b));                           \
  }                                                               \
  template<typename T>                                            \
  TVM_XINLINE RTYPE operator OP (half a, T b) {                   \
    return RTYPE(float(a) OP float(b));                           \
  }                                                               \
  template<typename T>                                            \
  TVM_XINLINE RTYPE operator OP (T a, half b) {                   \
    return RTYPE(float(a) OP float(b));                           \
  }

#define TVM_HALF_ASSIGNOP(AOP, OP)                                \
  template<typename T>                                            \
  TVM_XINLINE half operator AOP (const T& a) {                    \
    return *this = half(float(*this) OP float(a));                \
  }                                                               \
  template<typename T>                                            \
  TVM_XINLINE half operator AOP (const volatile T& a) volatile {  \
    return *this = half(float(*this) OP float(a));                \
  }

class TVM_ALIGNED(2) half {
 public:
  uint16_t half_;

  static TVM_XINLINE half Binary(uint16_t value) {
    half res;
    res.half_ = value;
    return res;
  }

  TVM_XINLINE half() {}

  TVM_XINLINE half(const float& value) { constructor(value); }
  TVM_XINLINE explicit half(const double& value) { constructor(value); }
  TVM_XINLINE explicit half(const int8_t& value) { constructor(value); }
  TVM_XINLINE explicit half(const uint8_t& value) { constructor(value); }
  TVM_XINLINE explicit half(const int32_t& value) { constructor(value); }
  TVM_XINLINE explicit half(const uint32_t& value) { constructor(value); }
  TVM_XINLINE explicit half(const long long& value) { constructor(value); }
  TVM_XINLINE explicit half(const uint64_t& value) { constructor(value); }

  TVM_XINLINE operator float() const {                          \
    return float(half2float(half_));                            \
  }                                                             \
  TVM_XINLINE operator float() const volatile {                 \
    return float(half2float(half_));                            \
  }


  TVM_HALF_ASSIGNOP(+=, +)
  TVM_HALF_ASSIGNOP(-=, -)
  TVM_HALF_ASSIGNOP(*=, *)
  TVM_HALF_ASSIGNOP(/=, /)

  TVM_XINLINE half operator+() {
    return *this;
  }

  TVM_XINLINE half operator-() {
    return half(-float(*this));
  }

  TVM_XINLINE half operator=(const half& a) {
    half_ = a.half_;
    return a;
  }

  template<typename T>
  TVM_XINLINE half operator=(const T& a) {
    return *this = half(a);
  }

  TVM_XINLINE half operator=(const half& a) volatile {
    half_ = a.half_;
    return a;
  }

  template<typename T>
  TVM_XINLINE half operator=(const T& a) volatile {
    return *this = half(a);
  }

 private:
  union Bits {
    float f;
    int32_t si;
    uint32_t ui;
  };

  static int const fp16FractionBits = 10;
  static int const fp32FractionBits = 23;
  static int32_t const fp32FractionMask = ~(~0u << fp32FractionBits);   // == 0x7fffff
  static int32_t const fp32HiddenBit = 1 << fp32FractionBits;   // == 0x800000
  static int const shift = fp32FractionBits - fp16FractionBits;   // == 13
  static int const shiftSign = 16;
  static int32_t const expAdjust = 127 - 15;   // exp32-127 = exp16-15, so exp16 = exp32 - (127-15)

  static int32_t const infN = 0x7F800000;   // flt32 infinity
  static int32_t const maxN = 0x477FFFFF;   // max flt32 that's a flt16 normal after >> by shift
  static int32_t const minN = 0x38800000;   // min flt16 normal as a flt32
  static int32_t const maxZ = 0x33000000;   // max fp32 number that's still rounded to zero in fp16
  static int32_t const signN = 0x80000000;  // flt32 sign bit

  static int32_t const infC = infN >> shift;
  static int32_t const nanN = (infC + 1) << shift;   // minimum flt16 nan as a flt32
  static int32_t const maxC = maxN >> shift;
  static int32_t const minC = minN >> shift;
  static int32_t const signC = signN >> shiftSign;  // flt16 sign bit

  static int32_t const mulN = 0x52000000;  // (1 << 23) / minN
  static int32_t const mulC = 0x33800000;  // minN / (1 << (23 - shift))

  static int32_t const subC = 0x003FF;  // max flt32 subnormal down shifted
  static int32_t const norC = 0x00400;  // min flt32 normal down shifted

  static int32_t const maxD = infC - maxC - 1;
  static int32_t const minD = minC - subC - 1;

  TVM_XINLINE uint16_t float2half(const float& value) const {
    Bits v;
    v.f = value;
    uint32_t sign = v.si & signN;    // grab sign bit
    v.si ^= sign;                    // clear sign bit from v
    sign >>= shiftSign;              // logical shift sign to fp16 position

    if (v.si <= maxZ) {
      // Handle eventual zeros here to ensure
      // vshift will not exceed 32 below.
      v.ui = 0;
    } else if (v.si < minN) {
      // Handle denorms
      uint32_t exp32 = v.ui >> fp32FractionBits;
      int32_t exp16 = exp32 - expAdjust;
      // If exp16 == 0 (just into the denorm range), then significant should be shifted right 1.
      // Smaller (so negative) exp16 values should result in greater right shifts.
      uint32_t vshift = 1 - exp16;
      uint32_t significand = fp32HiddenBit | (v.ui & fp32FractionMask);
      v.ui = significand >> vshift;
      v.ui += (v.ui & 0x3fff) != 0x1000 || (significand & 0x7ff) ? 0x1000 : 0;
    } else if (v.si <= maxN) {
      // Handle norms
      v.ui += (v.ui & 0x3fff) != 0x1000 ? 0x1000 : 0;
      v.ui -= expAdjust << fp32FractionBits;
    } else if (v.si <= infN) {
      v.si = infN;
    } else if (v.si < nanN) {
      v.si = nanN;
    }

    v.ui >>= shift;
    return sign | (v.ui & 0x7fff);
  }

  // Same as above routine, except for addition of volatile keyword
  TVM_XINLINE uint16_t float2half(
    const volatile float& value) const volatile {
    Bits v;
    v.f = value;
    uint32_t sign = v.si & signN;    // grab sign bit
    v.si ^= sign;                    // clear sign bit from v
    sign >>= shiftSign;              // logical shift sign to fp16 position

    if (v.si <= maxZ) {
      // Handle eventual zeros here to ensure
      // vshift will not exceed 32 below.
      v.ui = 0;
    } else if (v.si < minN) {
      // Handle denorms
      uint32_t exp32 = v.ui >> fp32FractionBits;
      int32_t exp16 = exp32 - expAdjust;
      // If exp16 == 0 (just into the denorm range), then significant should be shifted right 1.
      // Smaller (so negative) exp16 values should result in greater right shifts.
      uint32_t vshift = 1 - exp16;
      uint32_t significand = fp32HiddenBit | (v.ui & fp32FractionMask);
      v.ui = significand >> vshift;
      v.ui += (v.ui & 0x3fff) != 0x1000 || (significand & 0x7ff) ? 0x1000 : 0;
    } else if (v.si <= maxN) {
      // Handle norms
      v.ui += (v.ui & 0x3fff) != 0x1000 ? 0x1000 : 0;
      v.ui -= expAdjust << fp32FractionBits;
    } else if (v.si <= infN) {
      v.si = infN;
    } else if (v.si < nanN) {
      v.si = nanN;
    }

    v.ui >>= shift;
    return sign | (v.ui & 0x7fff);
  }

  TVM_XINLINE float half2float(const uint16_t& value) const {
    Bits v;
    v.ui = value;
    int32_t sign = v.si & signC;
    v.si ^= sign;
    sign <<= shiftSign;
    v.si ^= ((v.si + minD) ^ v.si) & -(v.si > subC);
    v.si ^= ((v.si + maxD) ^ v.si) & -(v.si > maxC);
    Bits s;
    s.si = mulC;
    s.f *= v.si;
    int32_t mask = -(norC > v.si);
    v.si <<= shift;
    v.si ^= (s.si ^ v.si) & mask;
    v.si |= sign;
    return v.f;
  }

  TVM_XINLINE float half2float(
    const volatile uint16_t& value) const volatile {
    Bits v;
    v.ui = value;
    int32_t sign = v.si & signC;
    v.si ^= sign;
    sign <<= shiftSign;
    v.si ^= ((v.si + minD) ^ v.si) & -(v.si > subC);
    v.si ^= ((v.si + maxD) ^ v.si) & -(v.si > maxC);
    Bits s;
    s.si = mulC;
    s.f *= v.si;
    int32_t mask = -(norC > v.si);
    v.si <<= shift;
    v.si ^= (s.si ^ v.si) & mask;
    v.si |= sign;
    return v.f;
  }

  template<typename T>
  TVM_XINLINE void constructor(const T& value) {
    half_ = float2half(float(value));
  }
};

TVM_HALF_OPERATOR(half, +)
TVM_HALF_OPERATOR(half, -)
TVM_HALF_OPERATOR(half, *)
TVM_HALF_OPERATOR(half, /)
TVM_HALF_OPERATOR(bool, >)
TVM_HALF_OPERATOR(bool, <)
TVM_HALF_OPERATOR(bool, >=)
TVM_HALF_OPERATOR(bool, <=)

TVM_XINLINE half __float2half_rn(const float a) {
  return half(a);
}
#endif


// Pack two half values.
static inline __device__ __host__ unsigned
__pack_half2(const half x, const half y) {
  unsigned v0 = *((unsigned short *)&x);
  unsigned v1 = *((unsigned short *)&y);
  return (v1 << 16) | v0;
}

// Some fp16 math functions are not supported in cuda_fp16.h,
// so we define them here to make sure the generated CUDA code
// is valid.
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
#define CUDA_UNSUPPORTED_HALF_MATH_BINARY(HALF_MATH_NAME, FP32_MATH_NAME) \
static inline __device__ __host__ half HALF_MATH_NAME(half x, half y) {   \
  float tmp_x = __half2float(x);                                          \
  float tmp_y = __half2float(y);                                          \
  float result = FP32_MATH_NAME(tmp_x, tmp_y);                            \
  return __float2half(result);                                            \
}

#define CUDA_UNSUPPORTED_HALF_MATH_UNARY(HALF_MATH_NAME, FP32_MATH_NAME) \
static inline __device__ __host__ half HALF_MATH_NAME(half x) {          \
  float tmp_x = __half2float(x);                                         \
  float result = FP32_MATH_NAME(tmp_x);                                  \
  return __float2half(result);                                           \
}

CUDA_UNSUPPORTED_HALF_MATH_BINARY(hpow, powf)
CUDA_UNSUPPORTED_HALF_MATH_UNARY(htanh, tanhf)
CUDA_UNSUPPORTED_HALF_MATH_UNARY(htan, tanf)
CUDA_UNSUPPORTED_HALF_MATH_UNARY(hatan, atanf)
CUDA_UNSUPPORTED_HALF_MATH_UNARY(herf, erf)

#undef CUDA_UNSUPPORTED_HALF_MATH_BINARY
#undef CUDA_UNSUPPORTED_HALF_MATH_UNARY

#endif
#include <mma.h>

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(128) dense_kernel0(half* __restrict__ A, half* __restrict__ B, float* __restrict__ C) {
  nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, float> C_reindex_wmma_accumulator[4];
  uint2 A_reindex_shared_local[8];
  __shared__ half A_reindex_shared[4608];
  uint4 B_reindex_shared_local[4];
  __shared__ half B_reindex_shared[4608];
  nvcuda::wmma::fragment<nvcuda::wmma::matrix_a, 16, 16, 16, half, nvcuda::wmma::row_major> A_reindex_shared_wmma_matrix_a[4];
  nvcuda::wmma::fragment<nvcuda::wmma::matrix_b, 16, 16, 16, half, nvcuda::wmma::col_major> B_reindex_shared_wmma_matrix_b[16];
  extern __shared__ float C_shared_dyn[];
  nvcuda::wmma::fill_fragment(C_reindex_wmma_accumulator[0], 0.000000e+00f);
  nvcuda::wmma::fill_fragment(C_reindex_wmma_accumulator[1], 0.000000e+00f);
  nvcuda::wmma::fill_fragment(C_reindex_wmma_accumulator[2], 0.000000e+00f);
  nvcuda::wmma::fill_fragment(C_reindex_wmma_accumulator[3], 0.000000e+00f);
  A_reindex_shared_local[0] = *(uint2*)(A + ((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + ((((int)threadIdx.x) & 15) * 4)));
  A_reindex_shared_local[1] = *(uint2*)(A + (((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + ((((int)threadIdx.x) & 15) * 4)) + 8192));
  A_reindex_shared_local[2] = *(uint2*)(A + (((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + ((((int)threadIdx.x) & 15) * 4)) + 16384));
  A_reindex_shared_local[3] = *(uint2*)(A + (((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + ((((int)threadIdx.x) & 15) * 4)) + 24576));
  A_reindex_shared_local[4] = *(uint2*)(A + (((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + ((((int)threadIdx.x) & 15) * 4)) + 32768));
  A_reindex_shared_local[5] = *(uint2*)(A + (((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + ((((int)threadIdx.x) & 15) * 4)) + 40960));
  A_reindex_shared_local[6] = *(uint2*)(A + (((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + ((((int)threadIdx.x) & 15) * 4)) + 49152));
  A_reindex_shared_local[7] = *(uint2*)(A + (((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + ((((int)threadIdx.x) & 15) * 4)) + 57344));
  *(uint2*)(A_reindex_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4))) = A_reindex_shared_local[0];
  *(uint2*)(A_reindex_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 512)) = A_reindex_shared_local[1];
  *(uint2*)(A_reindex_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 1024)) = A_reindex_shared_local[2];
  *(uint2*)(A_reindex_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 1536)) = A_reindex_shared_local[3];
  *(uint2*)(A_reindex_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 2048)) = A_reindex_shared_local[4];
  *(uint2*)(A_reindex_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 2560)) = A_reindex_shared_local[5];
  *(uint2*)(A_reindex_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 3072)) = A_reindex_shared_local[6];
  *(uint2*)(A_reindex_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 3584)) = A_reindex_shared_local[7];
  B_reindex_shared_local[0] = *(uint4*)(B + ((((((((int)blockIdx.x) & 7) * 131072) + ((((int)blockIdx.y) & 1) * 65536)) + (((int)threadIdx.y) * 4096)) + ((((int)threadIdx.x) >> 3) * 1024)) + ((((int)threadIdx.x) & 7) * 8)));
  B_reindex_shared_local[1] = *(uint4*)(B + (((((((((int)blockIdx.x) & 7) * 131072) + ((((int)blockIdx.y) & 1) * 65536)) + (((int)threadIdx.y) * 4096)) + ((((int)threadIdx.x) >> 3) * 1024)) + ((((int)threadIdx.x) & 7) * 8)) + 16384));
  B_reindex_shared_local[2] = *(uint4*)(B + (((((((((int)blockIdx.x) & 7) * 131072) + ((((int)blockIdx.y) & 1) * 65536)) + (((int)threadIdx.y) * 4096)) + ((((int)threadIdx.x) >> 3) * 1024)) + ((((int)threadIdx.x) & 7) * 8)) + 32768));
  B_reindex_shared_local[3] = *(uint4*)(B + (((((((((int)blockIdx.x) & 7) * 131072) + ((((int)blockIdx.y) & 1) * 65536)) + (((int)threadIdx.y) * 4096)) + ((((int)threadIdx.x) >> 3) * 1024)) + ((((int)threadIdx.x) & 7) * 8)) + 49152));
  *(uint4*)(B_reindex_shared + ((((int)threadIdx.y) * 256) + (((int)threadIdx.x) * 8))) = B_reindex_shared_local[0];
  *(uint4*)(B_reindex_shared + (((((int)threadIdx.y) * 256) + (((int)threadIdx.x) * 8)) + 1024)) = B_reindex_shared_local[1];
  *(uint4*)(B_reindex_shared + (((((int)threadIdx.y) * 256) + (((int)threadIdx.x) * 8)) + 2048)) = B_reindex_shared_local[2];
  *(uint4*)(B_reindex_shared + (((((int)threadIdx.y) * 256) + (((int)threadIdx.x) * 8)) + 3072)) = B_reindex_shared_local[3];
  __syncthreads();
  nvcuda::wmma::load_matrix_sync(A_reindex_shared_wmma_matrix_a[0], (&(A_reindex_shared[(((int)threadIdx.y) * 1152)])), 72);
  nvcuda::wmma::load_matrix_sync(A_reindex_shared_wmma_matrix_a[1], (&(A_reindex_shared[((((int)threadIdx.y) * 1152) + 16)])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[0], (&(B_reindex_shared[0])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[1], (&(B_reindex_shared[16])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[2], (&(B_reindex_shared[1152])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[3], (&(B_reindex_shared[1168])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[4], (&(B_reindex_shared[2304])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[5], (&(B_reindex_shared[2320])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[6], (&(B_reindex_shared[3456])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[7], (&(B_reindex_shared[3472])), 72);
  __syncthreads();
  for (int ax_0_0 = 0; ax_0_0 < 15; ++ax_0_0) {
    A_reindex_shared_local[0] = *(uint2*)(A + ((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + (ax_0_0 * 64)) + ((((int)threadIdx.x) & 15) * 4)) + 64));
    A_reindex_shared_local[1] = *(uint2*)(A + ((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + (ax_0_0 * 64)) + ((((int)threadIdx.x) & 15) * 4)) + 8256));
    A_reindex_shared_local[2] = *(uint2*)(A + ((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + (ax_0_0 * 64)) + ((((int)threadIdx.x) & 15) * 4)) + 16448));
    A_reindex_shared_local[3] = *(uint2*)(A + ((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + (ax_0_0 * 64)) + ((((int)threadIdx.x) & 15) * 4)) + 24640));
    A_reindex_shared_local[4] = *(uint2*)(A + ((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + (ax_0_0 * 64)) + ((((int)threadIdx.x) & 15) * 4)) + 32832));
    A_reindex_shared_local[5] = *(uint2*)(A + ((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + (ax_0_0 * 64)) + ((((int)threadIdx.x) & 15) * 4)) + 41024));
    A_reindex_shared_local[6] = *(uint2*)(A + ((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + (ax_0_0 * 64)) + ((((int)threadIdx.x) & 15) * 4)) + 49216));
    A_reindex_shared_local[7] = *(uint2*)(A + ((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 4) * 1024)) + (ax_0_0 * 64)) + ((((int)threadIdx.x) & 15) * 4)) + 57408));
    *(uint2*)(A_reindex_shared + ((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4))) = A_reindex_shared_local[0];
    *(uint2*)(A_reindex_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 512)) = A_reindex_shared_local[1];
    *(uint2*)(A_reindex_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 1024)) = A_reindex_shared_local[2];
    *(uint2*)(A_reindex_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 1536)) = A_reindex_shared_local[3];
    *(uint2*)(A_reindex_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 2048)) = A_reindex_shared_local[4];
    *(uint2*)(A_reindex_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 2560)) = A_reindex_shared_local[5];
    *(uint2*)(A_reindex_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 3072)) = A_reindex_shared_local[6];
    *(uint2*)(A_reindex_shared + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 3584)) = A_reindex_shared_local[7];
    __syncthreads();
    nvcuda::wmma::load_matrix_sync(A_reindex_shared_wmma_matrix_a[2], (&(A_reindex_shared[((((int)threadIdx.y) * 1152) + 32)])), 72);
    nvcuda::wmma::load_matrix_sync(A_reindex_shared_wmma_matrix_a[3], (&(A_reindex_shared[((((int)threadIdx.y) * 1152) + 48)])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[8], (&(B_reindex_shared[32])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[9], (&(B_reindex_shared[48])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[10], (&(B_reindex_shared[1184])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[11], (&(B_reindex_shared[1200])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[12], (&(B_reindex_shared[2336])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[13], (&(B_reindex_shared[2352])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[14], (&(B_reindex_shared[3488])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[15], (&(B_reindex_shared[3504])), 72);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[0], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[0], C_reindex_wmma_accumulator[0]);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[0], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[0], C_reindex_wmma_accumulator[0]);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[1], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[2], C_reindex_wmma_accumulator[1]);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[1], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[2], C_reindex_wmma_accumulator[1]);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[2], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[4], C_reindex_wmma_accumulator[2]);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[2], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[4], C_reindex_wmma_accumulator[2]);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[3], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[6], C_reindex_wmma_accumulator[3]);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[3], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[6], C_reindex_wmma_accumulator[3]);
    B_reindex_shared_local[0] = *(uint4*)(B + ((((((((((int)blockIdx.x) & 7) * 131072) + ((((int)blockIdx.y) & 1) * 65536)) + (((int)threadIdx.y) * 4096)) + ((((int)threadIdx.x) >> 3) * 1024)) + (ax_0_0 * 64)) + ((((int)threadIdx.x) & 7) * 8)) + 64));
    B_reindex_shared_local[1] = *(uint4*)(B + ((((((((((int)blockIdx.x) & 7) * 131072) + ((((int)blockIdx.y) & 1) * 65536)) + (((int)threadIdx.y) * 4096)) + ((((int)threadIdx.x) >> 3) * 1024)) + (ax_0_0 * 64)) + ((((int)threadIdx.x) & 7) * 8)) + 16448));
    B_reindex_shared_local[2] = *(uint4*)(B + ((((((((((int)blockIdx.x) & 7) * 131072) + ((((int)blockIdx.y) & 1) * 65536)) + (((int)threadIdx.y) * 4096)) + ((((int)threadIdx.x) >> 3) * 1024)) + (ax_0_0 * 64)) + ((((int)threadIdx.x) & 7) * 8)) + 32832));
    B_reindex_shared_local[3] = *(uint4*)(B + ((((((((((int)blockIdx.x) & 7) * 131072) + ((((int)blockIdx.y) & 1) * 65536)) + (((int)threadIdx.y) * 4096)) + ((((int)threadIdx.x) >> 3) * 1024)) + (ax_0_0 * 64)) + ((((int)threadIdx.x) & 7) * 8)) + 49216));
    __syncthreads();
    *(uint4*)(B_reindex_shared + ((((int)threadIdx.y) * 256) + (((int)threadIdx.x) * 8))) = B_reindex_shared_local[0];
    *(uint4*)(B_reindex_shared + (((((int)threadIdx.y) * 256) + (((int)threadIdx.x) * 8)) + 1024)) = B_reindex_shared_local[1];
    *(uint4*)(B_reindex_shared + (((((int)threadIdx.y) * 256) + (((int)threadIdx.x) * 8)) + 2048)) = B_reindex_shared_local[2];
    *(uint4*)(B_reindex_shared + (((((int)threadIdx.y) * 256) + (((int)threadIdx.x) * 8)) + 3072)) = B_reindex_shared_local[3];
    nvcuda::wmma::load_matrix_sync(A_reindex_shared_wmma_matrix_a[0], (&(A_reindex_shared[(((int)threadIdx.y) * 1152)])), 72);
    nvcuda::wmma::load_matrix_sync(A_reindex_shared_wmma_matrix_a[1], (&(A_reindex_shared[((((int)threadIdx.y) * 1152) + 16)])), 72);
    __syncthreads();
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[0], (&(B_reindex_shared[0])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[1], (&(B_reindex_shared[16])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[2], (&(B_reindex_shared[1152])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[3], (&(B_reindex_shared[1168])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[4], (&(B_reindex_shared[2304])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[5], (&(B_reindex_shared[2320])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[6], (&(B_reindex_shared[3456])), 72);
    nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[7], (&(B_reindex_shared[3472])), 72);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[0], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[8], C_reindex_wmma_accumulator[0]);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[0], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[8], C_reindex_wmma_accumulator[0]);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[1], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[10], C_reindex_wmma_accumulator[1]);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[1], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[10], C_reindex_wmma_accumulator[1]);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[2], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[12], C_reindex_wmma_accumulator[2]);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[2], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[12], C_reindex_wmma_accumulator[2]);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[3], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[14], C_reindex_wmma_accumulator[3]);
    nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[3], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[14], C_reindex_wmma_accumulator[3]);
  }
  nvcuda::wmma::load_matrix_sync(A_reindex_shared_wmma_matrix_a[2], (&(A_reindex_shared[((((int)threadIdx.y) * 1152) + 32)])), 72);
  nvcuda::wmma::load_matrix_sync(A_reindex_shared_wmma_matrix_a[3], (&(A_reindex_shared[((((int)threadIdx.y) * 1152) + 48)])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[8], (&(B_reindex_shared[32])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[9], (&(B_reindex_shared[48])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[10], (&(B_reindex_shared[1184])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[11], (&(B_reindex_shared[1200])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[12], (&(B_reindex_shared[2336])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[13], (&(B_reindex_shared[2352])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[14], (&(B_reindex_shared[3488])), 72);
  nvcuda::wmma::load_matrix_sync(B_reindex_shared_wmma_matrix_b[15], (&(B_reindex_shared[3504])), 72);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[0], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[0], C_reindex_wmma_accumulator[0]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[0], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[0], C_reindex_wmma_accumulator[0]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[1], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[2], C_reindex_wmma_accumulator[1]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[1], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[2], C_reindex_wmma_accumulator[1]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[2], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[4], C_reindex_wmma_accumulator[2]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[2], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[4], C_reindex_wmma_accumulator[2]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[3], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[6], C_reindex_wmma_accumulator[3]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[3], A_reindex_shared_wmma_matrix_a[0], B_reindex_shared_wmma_matrix_b[6], C_reindex_wmma_accumulator[3]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[0], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[8], C_reindex_wmma_accumulator[0]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[0], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[8], C_reindex_wmma_accumulator[0]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[1], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[10], C_reindex_wmma_accumulator[1]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[1], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[10], C_reindex_wmma_accumulator[1]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[2], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[12], C_reindex_wmma_accumulator[2]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[2], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[12], C_reindex_wmma_accumulator[2]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[3], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[14], C_reindex_wmma_accumulator[3]);
  nvcuda::wmma::mma_sync(C_reindex_wmma_accumulator[3], A_reindex_shared_wmma_matrix_a[2], B_reindex_shared_wmma_matrix_b[14], C_reindex_wmma_accumulator[3]);
  nvcuda::wmma::store_matrix_sync((&(C_shared_dyn[(((int)threadIdx.y) * 1152)])), C_reindex_wmma_accumulator[0], 72, nvcuda::wmma::mem_row_major);
  nvcuda::wmma::store_matrix_sync((&(C_shared_dyn[((((int)threadIdx.y) * 1152) + 16)])), C_reindex_wmma_accumulator[1], 72, nvcuda::wmma::mem_row_major);
  nvcuda::wmma::store_matrix_sync((&(C_shared_dyn[((((int)threadIdx.y) * 1152) + 32)])), C_reindex_wmma_accumulator[2], 72, nvcuda::wmma::mem_row_major);
  nvcuda::wmma::store_matrix_sync((&(C_shared_dyn[((((int)threadIdx.y) * 1152) + 48)])), C_reindex_wmma_accumulator[3], 72, nvcuda::wmma::mem_row_major);
  __syncthreads();
  *(float4*)(C + (((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + ((((int)threadIdx.y) & 1) * 8192)) + ((((int)threadIdx.x) >> 2) * 1024)) + ((((int)blockIdx.x) & 7) * 128)) + ((((int)blockIdx.y) & 1) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4))) = *(float4*)(C_shared_dyn + (((((((int)threadIdx.y) & 1) * 512) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)));
  *(float4*)(C + ((((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + ((((int)threadIdx.y) & 1) * 8192)) + ((((int)threadIdx.x) >> 2) * 1024)) + ((((int)blockIdx.x) & 7) * 128)) + ((((int)blockIdx.y) & 1) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + 32)) = *(float4*)(C_shared_dyn + ((((((((int)threadIdx.y) & 1) * 512) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + 32));
  *(float4*)(C + ((((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + ((((int)threadIdx.y) & 1) * 8192)) + ((((int)threadIdx.x) >> 2) * 1024)) + ((((int)blockIdx.x) & 7) * 128)) + ((((int)blockIdx.y) & 1) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + 16384)) = *(float4*)(C_shared_dyn + ((((((((int)threadIdx.y) & 1) * 512) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + 1024));
  *(float4*)(C + ((((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + ((((int)threadIdx.y) & 1) * 8192)) + ((((int)threadIdx.x) >> 2) * 1024)) + ((((int)blockIdx.x) & 7) * 128)) + ((((int)blockIdx.y) & 1) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + 16416)) = *(float4*)(C_shared_dyn + ((((((((int)threadIdx.y) & 1) * 512) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + 1056));
  *(float4*)(C + ((((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + ((((int)threadIdx.y) & 1) * 8192)) + ((((int)threadIdx.x) >> 2) * 1024)) + ((((int)blockIdx.x) & 7) * 128)) + ((((int)blockIdx.y) & 1) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + 32768)) = *(float4*)(C_shared_dyn + ((((((((int)threadIdx.y) & 1) * 512) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + 2048));
  *(float4*)(C + ((((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + ((((int)threadIdx.y) & 1) * 8192)) + ((((int)threadIdx.x) >> 2) * 1024)) + ((((int)blockIdx.x) & 7) * 128)) + ((((int)blockIdx.y) & 1) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + 32800)) = *(float4*)(C_shared_dyn + ((((((((int)threadIdx.y) & 1) * 512) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + 2080));
  *(float4*)(C + ((((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + ((((int)threadIdx.y) & 1) * 8192)) + ((((int)threadIdx.x) >> 2) * 1024)) + ((((int)blockIdx.x) & 7) * 128)) + ((((int)blockIdx.y) & 1) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + 49152)) = *(float4*)(C_shared_dyn + ((((((((int)threadIdx.y) & 1) * 512) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + 3072));
  *(float4*)(C + ((((((((((((int)blockIdx.x) >> 3) * 262144) + ((((int)blockIdx.y) >> 1) * 65536)) + ((((int)threadIdx.y) & 1) * 8192)) + ((((int)threadIdx.x) >> 2) * 1024)) + ((((int)blockIdx.x) & 7) * 128)) + ((((int)blockIdx.y) & 1) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + 49184)) = *(float4*)(C_shared_dyn + ((((((((int)threadIdx.y) & 1) * 512) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)threadIdx.y) >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + 3104));
}


